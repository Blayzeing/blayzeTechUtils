package classes.blayzeNet.nuclei;

/**
 * A fast, array-based network of sigmoid neurons.
 * This has editable weights, but no learning algorithms of any kind, and was intended for use with evolutionary genetic networks.
 * The network is modelled as a fully-connected nxm network of sigmoid neurons.
 */
 
 import classes.math.MoarMath;

public class FullyConnectedSigmoidLattice {
	
	private double[][] weights, inputWeights, outputWeights;
	private int layerWidth, inputCount, outputCount, networkDepth;
	
	/**
	 * Class constructor.
	 * One time use to construct the lattice.
	 * @param	numberOfInputs		number of inputs this lattice will have
	 * @param	width						how wide each layer of this network will be (sometimes called height)
	 * @param	depth						how many layers deep will this network go
	 * @param	numberOfOutputs	number of outputs this lattice will have
	 */
	public FullyConnectedSigmoidLattice (int numberOfInputs, int width, int depth, int numberOfOutputs)
	{
		weights = new double[depth-1][width*width];
		inputWeights = new double[numberOfInputs * width];
		outputWeights = new double[numberOfOutputs * width];
		layerWidth = width;
		inputCount = numberOfInputs;
		outputCount = numberOfOutputs;
		networkDepth = depth;
	}

	public void randomizeWeights(double min, double max)
	{
		for(int i = 0; i<weights.length; i++)
			for(int o = 0; o<weights[i].length; o++)
				weights[i][o] = Math.random() * (max-min) + min;
	}
	public void randomizeWeights()
	{
		randomizeWeights(-1,1);
	}

	/**
	 * Propagates the given inputs through the network.
	 * @return	double[]	an array that corresponds to the output values generated by this array
	 */
	public double[] forwardPropagate (double[] inputs)
	{
		//Set up temporary storage of neuron values per weight level
		double[] currentLayerValues = new double[layerWidth];
		for(int i = 0; i<layerWidth; i++)
		{
			double sum = 0;// Sum for this neuron of it's weights times it's inputs
			for(int o = 0; o<inputCount; o++)
			{
				sum = sum + inputWeights[inputCount* i + o] * inputs[o] 
			}
			currentLayerValues[i] = MoarMath.logit(sum);
		}
		//Loop through each layer of the lattice, calculating the next layer values
		for(int layer = 0; layer<networkDepth; layer++)
		{
			// Create a copy of the previous layer for use in the next
			double[] previousValues = Arrays.copyOf(currentLayerValues, layerWidth);
			for(int i = 0; i<layerWidth; i++)
			{
				double sum = 0;// Sum of each neuron in the last layer multiplied by this neuron's weights
				for(int o = 0; o < layerWidth; o++)
				{
					sum = sum + weights[layer][layerWidth * i + o] * previousValues[o];
				}
				currentLayerValues[i] = MoarMath.logit(sum);
			}
		}
		// Finally, loop through each element in currentLayerValues (the last full layer of neurons)
		// and multiply each layer with it's value in outputWeights, sum and logit them.
		double[] output = new double[outputCount];
		for(int i = 0; i < outputCount; i++)
		{
			double sum = 0;
			for(int o = 0; o<layerWidth; o++)
			{
				sum = sum + currentLayerValues[o] * outputWeights[layerWidth * i + o];
			}
			output[i] = MoarMath.logit(sum);
		}
		return(output);
	}
}